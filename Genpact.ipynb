{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split as splt\n",
    "# for handling the outliers\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.optimizers import Adadelta\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import save_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 IMPORTING THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id  week  checkout_price  base_price  num_orders  101  102  104  106  \\\n",
      "0  1379560     1          136.83      152.29         177    0    0    0    0   \n",
      "1  1466964     1          136.83      135.83         270    0    0    0    0   \n",
      "2  1346989     1          134.86      135.86         189    0    0    0    0   \n",
      "3  1338232     1          339.50      437.53          54    0    0    0    0   \n",
      "4  1448490     1          243.50      242.50          40    0    0    0    0   \n",
      "\n",
      "   108  ...  2640  2664  2704  2707  2760  2826  2867  2956  1  1  \n",
      "0    0  ...     0     0     0     0     0     0     0     0  0  0  \n",
      "1    0  ...     0     0     0     0     0     0     0     0  0  0  \n",
      "2    0  ...     0     0     0     0     0     0     0     0  0  0  \n",
      "3    0  ...     0     0     0     0     0     0     0     0  0  0  \n",
      "4    0  ...     0     0     0     0     0     0     0     0  0  0  \n",
      "\n",
      "[5 rows x 133 columns]\n",
      "             id  week  checkout_price  base_price  num_orders  101  102  104  \\\n",
      "456543  1271326   145          484.09      484.09          68    0    0    0   \n",
      "456544  1062036   145          482.09      482.09          42    0    0    0   \n",
      "456545  1110849   145          237.68      321.07         501    0    0    0   \n",
      "456546  1147725   145          243.50      313.34         729    0    0    0   \n",
      "456547  1361984   145          292.03      290.03         162    0    0    0   \n",
      "\n",
      "        106  108  ...  2640  2664  2704  2707  2760  2826  2867  2956  1  1  \n",
      "456543    0    0  ...     0     0     0     0     0     0     0     0  0  0  \n",
      "456544    0    0  ...     0     0     0     0     0     0     0     0  0  0  \n",
      "456545    0    0  ...     0     1     0     0     0     0     0     0  0  0  \n",
      "456546    0    0  ...     0     0     0     0     0     0     0     0  0  0  \n",
      "456547    0    0  ...     0     0     0     0     0     0     0     0  0  0  \n",
      "\n",
      "[5 rows x 133 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train.head())\n",
    "print(train.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id  week  checkout_price  base_price  101  102  104  106  108  109  \\\n",
      "0  1028232   146          158.11      159.11    0    0    0    0    0    0   \n",
      "1  1127204   146          160.11      159.11    0    0    0    0    0    0   \n",
      "2  1212707   146          157.14      159.14    0    0    0    0    0    0   \n",
      "3  1082698   146          162.02      162.02    0    0    0    0    0    0   \n",
      "4  1400926   146          163.93      163.93    0    0    0    0    0    0   \n",
      "\n",
      "   ...  2640  2664  2704  2707  2760  2826  2867  2956  1  1  \n",
      "0  ...     0     0     0     0     0     0     0     0  0  0  \n",
      "1  ...     0     0     0     0     0     0     0     0  0  0  \n",
      "2  ...     0     0     0     0     0     0     0     0  0  0  \n",
      "3  ...     0     0     0     0     0     0     0     0  0  0  \n",
      "4  ...     0     0     0     0     0     0     0     0  0  0  \n",
      "\n",
      "[5 rows x 132 columns]\n",
      "            id  week  checkout_price  base_price  101  102  104  106  108  \\\n",
      "32568  1250239   155          482.09      484.09    0    0    0    0    0   \n",
      "32569  1039516   155          483.09      483.09    0    0    0    0    0   \n",
      "32570  1158107   155          322.07      323.07    0    0    0    0    0   \n",
      "32571  1444235   155          322.07      323.07    0    0    0    0    0   \n",
      "32572  1291286   155          276.45      276.45    0    0    0    0    0   \n",
      "\n",
      "       109  ...  2640  2664  2704  2707  2760  2826  2867  2956  1  1  \n",
      "32568    0  ...     0     0     0     0     0     0     0     0  0  0  \n",
      "32569    0  ...     0     0     0     0     0     0     0     0  0  0  \n",
      "32570    0  ...     0     1     0     0     0     0     0     0  0  0  \n",
      "32571    0  ...     0     0     0     0     0     0     0     0  0  0  \n",
      "32572    0  ...     0     0     0     0     0     0     0     0  0  0  \n",
      "\n",
      "[5 rows x 132 columns]\n"
     ]
    }
   ],
   "source": [
    "print(test.head())\n",
    "print(test.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 FEATURE ENGINEERING\n",
    "- **2.1. CHANGING THE DATA TYPE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop('id,week,num_orders,checkout_price,base_price'.split(','),axis=1)\n",
    "Y = test.drop('id,week,checkout_price,base_price'.split(','),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.astype(str)\n",
    "Y = Y.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       checkout_price     base_price            101            102  \\\n",
      "count   423603.000000  423603.000000  423603.000000  423603.000000   \n",
      "mean       340.704680     361.137211       0.013765       0.011317   \n",
      "std        153.812665     161.269561       0.116515       0.105779   \n",
      "min          2.970000      55.350000       0.000000       0.000000   \n",
      "25%        241.530000     244.500000       0.000000       0.000000   \n",
      "50%        309.430000     317.220000       0.000000       0.000000   \n",
      "75%        446.230000     470.510000       0.000000       0.000000   \n",
      "max        767.330000     767.330000       1.000000       1.000000   \n",
      "\n",
      "                 104            106            108           109  \\\n",
      "count  423603.000000  423603.000000  423603.000000  423603.00000   \n",
      "mean        0.013593       0.012944       0.014211       0.01368   \n",
      "std         0.115794       0.113032       0.118362       0.11616   \n",
      "min         0.000000       0.000000       0.000000       0.00000   \n",
      "25%         0.000000       0.000000       0.000000       0.00000   \n",
      "50%         0.000000       0.000000       0.000000       0.00000   \n",
      "75%         0.000000       0.000000       0.000000       0.00000   \n",
      "max         1.000000       1.000000       1.000000       1.00000   \n",
      "\n",
      "                  11           110  ...           2640           2664  \\\n",
      "count  423603.000000  423603.00000  ...  423603.000000  423603.000000   \n",
      "mean        0.013066       0.01280  ...       0.025082       0.021400   \n",
      "std         0.113560       0.11241  ...       0.156376       0.144713   \n",
      "min         0.000000       0.00000  ...       0.000000       0.000000   \n",
      "25%         0.000000       0.00000  ...       0.000000       0.000000   \n",
      "50%         0.000000       0.00000  ...       0.000000       0.000000   \n",
      "75%         0.000000       0.00000  ...       0.000000       0.000000   \n",
      "max         1.000000       1.00000  ...       1.000000       1.000000   \n",
      "\n",
      "                2704           2707           2760           2826  \\\n",
      "count  423603.000000  423603.000000  423603.000000  423603.000000   \n",
      "mean        0.022986       0.019584       0.023878       0.022245   \n",
      "std         0.149859       0.138567       0.152671       0.147479   \n",
      "min         0.000000       0.000000       0.000000       0.000000   \n",
      "25%         0.000000       0.000000       0.000000       0.000000   \n",
      "50%         0.000000       0.000000       0.000000       0.000000   \n",
      "75%         0.000000       0.000000       0.000000       0.000000   \n",
      "max         1.000000       1.000000       1.000000       1.000000   \n",
      "\n",
      "                2867           2956              1              1  \n",
      "count  423603.000000  423603.000000  423603.000000  423603.000000  \n",
      "mean        0.019027       0.007830       0.065347       0.087842  \n",
      "std         0.136621       0.088143       0.247137       0.283065  \n",
      "min         0.000000       0.000000       0.000000       0.000000  \n",
      "25%         0.000000       0.000000       0.000000       0.000000  \n",
      "50%         0.000000       0.000000       0.000000       0.000000  \n",
      "75%         0.000000       0.000000       0.000000       0.000000  \n",
      "max         1.000000       1.000000       1.000000       1.000000  \n",
      "\n",
      "[8 rows x 130 columns]\n",
      "count    423603.000000\n",
      "mean        182.205542\n",
      "std         169.519667\n",
      "min          13.000000\n",
      "25%          53.000000\n",
      "50%         122.000000\n",
      "75%         270.000000\n",
      "max         729.000000\n",
      "Name: num_orders, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(X.describe())\n",
    "print(Y.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['center_id,meal_id,emailer_for_promotion,homepage_featured'.split(',')] = X\n",
    "test['center_id,meal_id,emailer_for_promotion,homepage_featured'.split(',')] = Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>week</th>\n",
       "      <th>checkout_price</th>\n",
       "      <th>base_price</th>\n",
       "      <th>num_orders</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>104</th>\n",
       "      <th>106</th>\n",
       "      <th>108</th>\n",
       "      <th>...</th>\n",
       "      <th>2640</th>\n",
       "      <th>2664</th>\n",
       "      <th>2704</th>\n",
       "      <th>2707</th>\n",
       "      <th>2760</th>\n",
       "      <th>2826</th>\n",
       "      <th>2867</th>\n",
       "      <th>2956</th>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.236030e+05</td>\n",
       "      <td>423603.000000</td>\n",
       "      <td>423603.000000</td>\n",
       "      <td>423603.000000</td>\n",
       "      <td>423603.000000</td>\n",
       "      <td>423603.000000</td>\n",
       "      <td>423603.000000</td>\n",
       "      <td>423603.000000</td>\n",
       "      <td>423603.000000</td>\n",
       "      <td>423603.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>423603.000000</td>\n",
       "      <td>423603.000000</td>\n",
       "      <td>423603.000000</td>\n",
       "      <td>423603.000000</td>\n",
       "      <td>423603.000000</td>\n",
       "      <td>423603.000000</td>\n",
       "      <td>423603.000000</td>\n",
       "      <td>423603.000000</td>\n",
       "      <td>423603.000000</td>\n",
       "      <td>423603.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.250083e+06</td>\n",
       "      <td>74.904432</td>\n",
       "      <td>340.704680</td>\n",
       "      <td>361.137211</td>\n",
       "      <td>182.205542</td>\n",
       "      <td>0.013765</td>\n",
       "      <td>0.011317</td>\n",
       "      <td>0.013593</td>\n",
       "      <td>0.012944</td>\n",
       "      <td>0.014211</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025082</td>\n",
       "      <td>0.021400</td>\n",
       "      <td>0.022986</td>\n",
       "      <td>0.019584</td>\n",
       "      <td>0.023878</td>\n",
       "      <td>0.022245</td>\n",
       "      <td>0.019027</td>\n",
       "      <td>0.007830</td>\n",
       "      <td>0.065347</td>\n",
       "      <td>0.087842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.443480e+05</td>\n",
       "      <td>41.587735</td>\n",
       "      <td>153.812665</td>\n",
       "      <td>161.269561</td>\n",
       "      <td>169.519667</td>\n",
       "      <td>0.116515</td>\n",
       "      <td>0.105779</td>\n",
       "      <td>0.115794</td>\n",
       "      <td>0.113032</td>\n",
       "      <td>0.118362</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156376</td>\n",
       "      <td>0.144713</td>\n",
       "      <td>0.149859</td>\n",
       "      <td>0.138567</td>\n",
       "      <td>0.152671</td>\n",
       "      <td>0.147479</td>\n",
       "      <td>0.136621</td>\n",
       "      <td>0.088143</td>\n",
       "      <td>0.247137</td>\n",
       "      <td>0.283065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000e+06</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.970000</td>\n",
       "      <td>55.350000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.125012e+06</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>241.530000</td>\n",
       "      <td>244.500000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.250096e+06</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>309.430000</td>\n",
       "      <td>317.220000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.375126e+06</td>\n",
       "      <td>111.000000</td>\n",
       "      <td>446.230000</td>\n",
       "      <td>470.510000</td>\n",
       "      <td>270.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.499999e+06</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>767.330000</td>\n",
       "      <td>767.330000</td>\n",
       "      <td>729.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 133 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id           week  checkout_price     base_price  \\\n",
       "count  4.236030e+05  423603.000000   423603.000000  423603.000000   \n",
       "mean   1.250083e+06      74.904432      340.704680     361.137211   \n",
       "std    1.443480e+05      41.587735      153.812665     161.269561   \n",
       "min    1.000000e+06       1.000000        2.970000      55.350000   \n",
       "25%    1.125012e+06      39.000000      241.530000     244.500000   \n",
       "50%    1.250096e+06      76.000000      309.430000     317.220000   \n",
       "75%    1.375126e+06     111.000000      446.230000     470.510000   \n",
       "max    1.499999e+06     145.000000      767.330000     767.330000   \n",
       "\n",
       "          num_orders            101            102            104  \\\n",
       "count  423603.000000  423603.000000  423603.000000  423603.000000   \n",
       "mean      182.205542       0.013765       0.011317       0.013593   \n",
       "std       169.519667       0.116515       0.105779       0.115794   \n",
       "min        13.000000       0.000000       0.000000       0.000000   \n",
       "25%        53.000000       0.000000       0.000000       0.000000   \n",
       "50%       122.000000       0.000000       0.000000       0.000000   \n",
       "75%       270.000000       0.000000       0.000000       0.000000   \n",
       "max       729.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "                 106            108  ...           2640           2664  \\\n",
       "count  423603.000000  423603.000000  ...  423603.000000  423603.000000   \n",
       "mean        0.012944       0.014211  ...       0.025082       0.021400   \n",
       "std         0.113032       0.118362  ...       0.156376       0.144713   \n",
       "min         0.000000       0.000000  ...       0.000000       0.000000   \n",
       "25%         0.000000       0.000000  ...       0.000000       0.000000   \n",
       "50%         0.000000       0.000000  ...       0.000000       0.000000   \n",
       "75%         0.000000       0.000000  ...       0.000000       0.000000   \n",
       "max         1.000000       1.000000  ...       1.000000       1.000000   \n",
       "\n",
       "                2704           2707           2760           2826  \\\n",
       "count  423603.000000  423603.000000  423603.000000  423603.000000   \n",
       "mean        0.022986       0.019584       0.023878       0.022245   \n",
       "std         0.149859       0.138567       0.152671       0.147479   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "                2867           2956              1              1  \n",
       "count  423603.000000  423603.000000  423603.000000  423603.000000  \n",
       "mean        0.019027       0.007830       0.065347       0.087842  \n",
       "std         0.136621       0.088143       0.247137       0.283065  \n",
       "min         0.000000       0.000000       0.000000       0.000000  \n",
       "25%         0.000000       0.000000       0.000000       0.000000  \n",
       "50%         0.000000       0.000000       0.000000       0.000000  \n",
       "75%         0.000000       0.000000       0.000000       0.000000  \n",
       "max         1.000000       1.000000       1.000000       1.000000  \n",
       "\n",
       "[8 rows x 133 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>week</th>\n",
       "      <th>checkout_price</th>\n",
       "      <th>base_price</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>104</th>\n",
       "      <th>106</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>...</th>\n",
       "      <th>2640</th>\n",
       "      <th>2664</th>\n",
       "      <th>2704</th>\n",
       "      <th>2707</th>\n",
       "      <th>2760</th>\n",
       "      <th>2826</th>\n",
       "      <th>2867</th>\n",
       "      <th>2956</th>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.257300e+04</td>\n",
       "      <td>32573.000000</td>\n",
       "      <td>32573.000000</td>\n",
       "      <td>32573.000000</td>\n",
       "      <td>32573.000000</td>\n",
       "      <td>32573.000000</td>\n",
       "      <td>32573.000000</td>\n",
       "      <td>32573.000000</td>\n",
       "      <td>32573.000000</td>\n",
       "      <td>32573.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>32573.000000</td>\n",
       "      <td>32573.000000</td>\n",
       "      <td>32573.000000</td>\n",
       "      <td>32573.000000</td>\n",
       "      <td>32573.000000</td>\n",
       "      <td>32573.000000</td>\n",
       "      <td>32573.000000</td>\n",
       "      <td>32573.000000</td>\n",
       "      <td>32573.000000</td>\n",
       "      <td>32573.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.248476e+06</td>\n",
       "      <td>150.477819</td>\n",
       "      <td>341.854440</td>\n",
       "      <td>356.493615</td>\n",
       "      <td>0.013140</td>\n",
       "      <td>0.010438</td>\n",
       "      <td>0.014491</td>\n",
       "      <td>0.012587</td>\n",
       "      <td>0.014951</td>\n",
       "      <td>0.013447</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022565</td>\n",
       "      <td>0.023639</td>\n",
       "      <td>0.022227</td>\n",
       "      <td>0.023455</td>\n",
       "      <td>0.021859</td>\n",
       "      <td>0.023639</td>\n",
       "      <td>0.016179</td>\n",
       "      <td>0.013078</td>\n",
       "      <td>0.066435</td>\n",
       "      <td>0.081356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.441580e+05</td>\n",
       "      <td>2.864072</td>\n",
       "      <td>153.893886</td>\n",
       "      <td>155.150101</td>\n",
       "      <td>0.113875</td>\n",
       "      <td>0.101634</td>\n",
       "      <td>0.119503</td>\n",
       "      <td>0.111486</td>\n",
       "      <td>0.121359</td>\n",
       "      <td>0.115179</td>\n",
       "      <td>...</td>\n",
       "      <td>0.148513</td>\n",
       "      <td>0.151925</td>\n",
       "      <td>0.147423</td>\n",
       "      <td>0.151346</td>\n",
       "      <td>0.146224</td>\n",
       "      <td>0.151925</td>\n",
       "      <td>0.126166</td>\n",
       "      <td>0.113612</td>\n",
       "      <td>0.249045</td>\n",
       "      <td>0.273385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000085e+06</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>67.900000</td>\n",
       "      <td>89.240000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.123969e+06</td>\n",
       "      <td>148.000000</td>\n",
       "      <td>214.430000</td>\n",
       "      <td>243.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.247296e+06</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>320.130000</td>\n",
       "      <td>321.130000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.372971e+06</td>\n",
       "      <td>153.000000</td>\n",
       "      <td>446.230000</td>\n",
       "      <td>455.930000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.499996e+06</td>\n",
       "      <td>155.000000</td>\n",
       "      <td>1113.620000</td>\n",
       "      <td>1112.620000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 132 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id          week  checkout_price    base_price           101  \\\n",
       "count  3.257300e+04  32573.000000    32573.000000  32573.000000  32573.000000   \n",
       "mean   1.248476e+06    150.477819      341.854440    356.493615      0.013140   \n",
       "std    1.441580e+05      2.864072      153.893886    155.150101      0.113875   \n",
       "min    1.000085e+06    146.000000       67.900000     89.240000      0.000000   \n",
       "25%    1.123969e+06    148.000000      214.430000    243.500000      0.000000   \n",
       "50%    1.247296e+06    150.000000      320.130000    321.130000      0.000000   \n",
       "75%    1.372971e+06    153.000000      446.230000    455.930000      0.000000   \n",
       "max    1.499996e+06    155.000000     1113.620000   1112.620000      1.000000   \n",
       "\n",
       "                102           104           106           108           109  \\\n",
       "count  32573.000000  32573.000000  32573.000000  32573.000000  32573.000000   \n",
       "mean       0.010438      0.014491      0.012587      0.014951      0.013447   \n",
       "std        0.101634      0.119503      0.111486      0.121359      0.115179   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "       ...          2640          2664          2704          2707  \\\n",
       "count  ...  32573.000000  32573.000000  32573.000000  32573.000000   \n",
       "mean   ...      0.022565      0.023639      0.022227      0.023455   \n",
       "std    ...      0.148513      0.151925      0.147423      0.151346   \n",
       "min    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "25%    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "50%    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "75%    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "max    ...      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "               2760          2826          2867          2956             1  \\\n",
       "count  32573.000000  32573.000000  32573.000000  32573.000000  32573.000000   \n",
       "mean       0.021859      0.023639      0.016179      0.013078      0.066435   \n",
       "std        0.146224      0.151925      0.126166      0.113612      0.249045   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                  1  \n",
       "count  32573.000000  \n",
       "mean       0.081356  \n",
       "std        0.273385  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.000000  \n",
       "75%        0.000000  \n",
       "max        1.000000  \n",
       "\n",
       "[8 rows x 132 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **REMOVING OUTLIERS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier(data):\n",
    "    qmax = {}\n",
    "    qmin = {}\n",
    "    for i in list(data.columns):\n",
    "        if(np.issubdtype(data[i].dtype, np.number) == True):\n",
    "            iqr = data[i].quantile(0.75) - data[i].quantile(0.25)\n",
    "            qmax[i] = data[i].quantile(0.75) + 1.5*iqr\n",
    "            qmin[i] = data[i].quantile(0.25) - 1.5*iqr\n",
    "\n",
    "    for i in list(data.columns):\n",
    "        if(np.issubdtype(data[i].dtype, np.number) == True):\n",
    "            data = data.drop(data[(data[i] > qmax[i]) | (data[i] < qmin[i])].index)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(423603, 9)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = outlier(train)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **2.2 CREATING DUMMY VARIABLES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = train\n",
    "for i in 'center_id,meal_id,emailer_for_promotion,homepage_featured'.split(','):\n",
    "    data = pd.get_dummies(dataset[i],drop_first=True)\n",
    "    dataset = pd.concat([dataset,data],axis=1)\n",
    "train = dataset\n",
    "train.drop('center_id,meal_id,emailer_for_promotion,homepage_featured'.split(','),axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = test\n",
    "for i in 'center_id,meal_id,emailer_for_promotion,homepage_featured'.split(','):\n",
    "    data = pd.get_dummies(dataset[i],drop_first=True)\n",
    "    dataset = pd.concat([dataset,data],axis=1)\n",
    "test = dataset\n",
    "test.drop('center_id,meal_id,emailer_for_promotion,homepage_featured'.split(','),axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 423603 entries, 0 to 456547\n",
      "Columns: 133 entries, id to 1\n",
      "dtypes: float64(2), int64(3), uint8(128)\n",
      "memory usage: 71.1 MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32573 entries, 0 to 32572\n",
      "Columns: 132 entries, id to 1\n",
      "dtypes: float64(2), int64(2), uint8(128)\n",
      "memory usage: 5.0 MB\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train = train.drop(['id','week'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 SEPARATING THE INDEPENDENT AND DEPENDENT VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = new_train.drop('num_orders',axis=1)\n",
    "Y = new_train['num_orders']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test = test.drop(['id','week'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 FEATURE SCALING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate array with shape (130, 423603) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-5e071f1c4be4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# fitting and transforming the X into feature scaling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# converting again into dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    637\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    661\u001b[0m         X = check_array(X, accept_sparse=('csr', 'csc'), copy=self.copy,\n\u001b[1;32m    662\u001b[0m                         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 663\u001b[0;31m                         force_all_finite='allow-nan')\n\u001b[0m\u001b[1;32m    664\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m         \u001b[0;31m# Even in the case of `with_mean=False`, we update the mean anyway\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataConversionWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmay_share_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray_orig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mmay_share_memory\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1993\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1994\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1995\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1996\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1997\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array_wrap__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.values_from_object\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mvalues\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5441\u001b[0m         \"\"\"\n\u001b[1;32m   5442\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_AXIS_REVERSED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5445\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mas_array\u001b[0;34m(self, transpose, items)\u001b[0m\n\u001b[1;32m    820\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interleave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtranspose\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m_interleave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    838\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"object\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0mitemmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate array with shape (130, 423603) and data type float64"
     ]
    }
   ],
   "source": [
    "data = X\n",
    "# fitting and transforming the X into feature scaling\n",
    "X = scale.fit_transform(X)\n",
    "# converting again into dataframe\n",
    "X = pd.DataFrame(X,columns=data.columns)\n",
    "data = new_test\n",
    "# fitting and transforming the test into feature scaling\n",
    "new_test = scale.transform(new_test)\n",
    "# converting again into dataframe\n",
    "new_test = pd.DataFrame(new_test,columns=data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.head())\n",
    "print(new_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 SPLITTING THE X DATA INTO TRAIN AND TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = splt(X, Y, test_size=0.9781, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 SAMPLE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_regressor():\n",
    "    regressor = Sequential()\n",
    "    regressor.add(Dense(units=66,kernel_initializer='normal',\n",
    "                     activation='relu',input_dim=130))\n",
    "    regressor.add(Dense(units=66,kernel_initializer='normal',\n",
    "                     activation='relu'))\n",
    "    regressor.add(Dense(units=66,kernel_initializer='normal',\n",
    "                     activation='relu'))\n",
    "    regressor.add(Dense(units=66,kernel_initializer='normal',\n",
    "                     activation='relu'))\n",
    "    regressor.add(Dense(units=66,kernel_initializer='normal',\n",
    "                     activation='relu'))\n",
    "    regressor.add(Dense(units=1,kernel_initializer='normal',\n",
    "                     activation='relu'))\n",
    "    regressor.compile(optimizer=Adadelta(),loss='mean_squared_logarithmic_error')\n",
    "    return regressor\n",
    "regressor = KerasRegressor(build_fn= build_regressor,batch_size=10,epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 423603 samples\n",
      "Epoch 1/150\n",
      "423603/423603 [==============================] - 108s 256us/sample - loss: 2.9836\n",
      "Epoch 2/150\n",
      "423603/423603 [==============================] - 108s 254us/sample - loss: 0.9957\n",
      "Epoch 3/150\n",
      "423603/423603 [==============================] - 108s 255us/sample - loss: 0.8897\n",
      "Epoch 4/150\n",
      "423603/423603 [==============================] - 108s 254us/sample - loss: 0.7882\n",
      "Epoch 5/150\n",
      "423603/423603 [==============================] - 109s 258us/sample - loss: 0.6921\n",
      "Epoch 6/150\n",
      "423603/423603 [==============================] - 108s 256us/sample - loss: 0.6137\n",
      "Epoch 7/150\n",
      "423603/423603 [==============================] - 108s 256us/sample - loss: 0.5601\n",
      "Epoch 8/150\n",
      "423603/423603 [==============================] - 107s 252us/sample - loss: 0.5233\n",
      "Epoch 9/150\n",
      "423603/423603 [==============================] - 112s 265us/sample - loss: 0.4976\n",
      "Epoch 10/150\n",
      "423603/423603 [==============================] - 107s 253us/sample - loss: 0.4796\n",
      "Epoch 11/150\n",
      "423603/423603 [==============================] - 108s 255us/sample - loss: 0.4666\n",
      "Epoch 12/150\n",
      "423603/423603 [==============================] - 108s 254us/sample - loss: 0.4571\n",
      "Epoch 13/150\n",
      "423603/423603 [==============================] - 107s 253us/sample - loss: 0.4497\n",
      "Epoch 14/150\n",
      "423603/423603 [==============================] - 108s 255us/sample - loss: 0.4437\n",
      "Epoch 15/150\n",
      "423603/423603 [==============================] - 109s 257us/sample - loss: 0.4387\n",
      "Epoch 16/150\n",
      "423603/423603 [==============================] - 108s 254us/sample - loss: 0.4344\n",
      "Epoch 17/150\n",
      "423603/423603 [==============================] - 108s 256us/sample - loss: 0.4306\n",
      "Epoch 18/150\n",
      "423603/423603 [==============================] - 108s 255us/sample - loss: 0.4272\n",
      "Epoch 19/150\n",
      "423603/423603 [==============================] - 108s 255us/sample - loss: 0.4239\n",
      "Epoch 20/150\n",
      "423603/423603 [==============================] - 108s 256us/sample - loss: 0.4209\n",
      "Epoch 21/150\n",
      "423603/423603 [==============================] - 108s 255us/sample - loss: 0.4181\n",
      "Epoch 22/150\n",
      "423603/423603 [==============================] - 109s 256us/sample - loss: 0.4155\n",
      "Epoch 23/150\n",
      "423603/423603 [==============================] - 106s 249us/sample - loss: 0.4129\n",
      "Epoch 24/150\n",
      "423603/423603 [==============================] - 108s 255us/sample - loss: 0.4105\n",
      "Epoch 25/150\n",
      "423603/423603 [==============================] - 108s 255us/sample - loss: 0.4081\n",
      "Epoch 26/150\n",
      "423603/423603 [==============================] - 108s 254us/sample - loss: 0.4058\n",
      "Epoch 27/150\n",
      "423603/423603 [==============================] - 108s 254us/sample - loss: 0.4036\n",
      "Epoch 28/150\n",
      "423603/423603 [==============================] - 108s 255us/sample - loss: 0.4014\n",
      "Epoch 29/150\n",
      "423603/423603 [==============================] - 111s 263us/sample - loss: 0.3993\n",
      "Epoch 30/150\n",
      "423603/423603 [==============================] - 109s 257us/sample - loss: 0.3972\n",
      "Epoch 31/150\n",
      "423603/423603 [==============================] - 108s 254us/sample - loss: 0.3951\n",
      "Epoch 32/150\n",
      "423603/423603 [==============================] - 109s 256us/sample - loss: 0.3931\n",
      "Epoch 33/150\n",
      "423603/423603 [==============================] - 109s 257us/sample - loss: 0.3911\n",
      "Epoch 34/150\n",
      "423603/423603 [==============================] - 108s 255us/sample - loss: 0.3892\n",
      "Epoch 35/150\n",
      "423603/423603 [==============================] - 108s 255us/sample - loss: 0.3872\n",
      "Epoch 36/150\n",
      "423603/423603 [==============================] - 108s 255us/sample - loss: 0.3853\n",
      "Epoch 37/150\n",
      "423603/423603 [==============================] - 108s 255us/sample - loss: 0.3835\n",
      "Epoch 38/150\n",
      "423603/423603 [==============================] - 106s 250us/sample - loss: 0.3816\n",
      "Epoch 39/150\n",
      "423603/423603 [==============================] - 108s 254us/sample - loss: 0.3799\n",
      "Epoch 40/150\n",
      "423603/423603 [==============================] - 107s 253us/sample - loss: 0.3781\n",
      "Epoch 41/150\n",
      "423603/423603 [==============================] - 107s 253us/sample - loss: 0.3763\n",
      "Epoch 42/150\n",
      "423603/423603 [==============================] - 108s 254us/sample - loss: 0.3746\n",
      "Epoch 43/150\n",
      "423603/423603 [==============================] - 107s 254us/sample - loss: 0.3729\n",
      "Epoch 44/150\n",
      "423603/423603 [==============================] - 103s 244us/sample - loss: 0.3712\n",
      "Epoch 45/150\n",
      "423603/423603 [==============================] - 100s 235us/sample - loss: 0.3695\n",
      "Epoch 46/150\n",
      "423603/423603 [==============================] - 106s 251us/sample - loss: 0.3678\n",
      "Epoch 47/150\n",
      "423603/423603 [==============================] - 103s 244us/sample - loss: 0.3662\n",
      "Epoch 48/150\n",
      "423603/423603 [==============================] - 100s 235us/sample - loss: 0.3646\n",
      "Epoch 49/150\n",
      "423603/423603 [==============================] - 100s 235us/sample - loss: 0.3629\n",
      "Epoch 50/150\n",
      "423603/423603 [==============================] - 105s 248us/sample - loss: 0.3613\n",
      "Epoch 51/150\n",
      "423603/423603 [==============================] - 104s 245us/sample - loss: 0.3598\n",
      "Epoch 52/150\n",
      "423603/423603 [==============================] - 100s 237us/sample - loss: 0.3582\n",
      "Epoch 53/150\n",
      "423603/423603 [==============================] - 100s 235us/sample - loss: 0.3566\n",
      "Epoch 54/150\n",
      "423603/423603 [==============================] - 100s 235us/sample - loss: 0.3551\n",
      "Epoch 55/150\n",
      "423603/423603 [==============================] - 101s 238us/sample - loss: 0.3536\n",
      "Epoch 56/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.3521\n",
      "Epoch 57/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.3507\n",
      "Epoch 58/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.3492\n",
      "Epoch 59/150\n",
      "423603/423603 [==============================] - 100s 237us/sample - loss: 0.3478\n",
      "Epoch 60/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.3464\n",
      "Epoch 61/150\n",
      "423603/423603 [==============================] - 101s 237us/sample - loss: 0.3451\n",
      "Epoch 62/150\n",
      "423603/423603 [==============================] - 100s 235us/sample - loss: 0.3437\n",
      "Epoch 63/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.3424\n",
      "Epoch 64/150\n",
      "423603/423603 [==============================] - 100s 237us/sample - loss: 0.3411\n",
      "Epoch 65/150\n",
      "423603/423603 [==============================] - 101s 237us/sample - loss: 0.3398\n",
      "Epoch 66/150\n",
      "423603/423603 [==============================] - 100s 235us/sample - loss: 0.3385\n",
      "Epoch 67/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.3373\n",
      "Epoch 68/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.3361\n",
      "Epoch 69/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.3349\n",
      "Epoch 70/150\n",
      "423603/423603 [==============================] - 100s 237us/sample - loss: 0.3337\n",
      "Epoch 71/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.3326\n",
      "Epoch 72/150\n",
      "423603/423603 [==============================] - 100s 235us/sample - loss: 0.3315\n",
      "Epoch 73/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.3304\n",
      "Epoch 74/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.3293\n",
      "Epoch 75/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.3283\n",
      "Epoch 76/150\n",
      "423603/423603 [==============================] - 100s 235us/sample - loss: 0.3273\n",
      "Epoch 77/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.3263\n",
      "Epoch 78/150\n",
      "423603/423603 [==============================] - 99s 235us/sample - loss: 0.3254\n",
      "Epoch 79/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.3244\n",
      "Epoch 80/150\n",
      "423603/423603 [==============================] - 100s 237us/sample - loss: 0.3235\n",
      "Epoch 81/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.3227\n",
      "Epoch 82/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.3218\n",
      "Epoch 83/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.3210\n",
      "Epoch 84/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.3202\n",
      "Epoch 85/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.3194\n",
      "Epoch 86/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.3186\n",
      "Epoch 87/150\n",
      "423603/423603 [==============================] - 100s 237us/sample - loss: 0.3179\n",
      "Epoch 88/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.3172\n",
      "Epoch 89/150\n",
      "423603/423603 [==============================] - 100s 235us/sample - loss: 0.3164\n",
      "Epoch 90/150\n",
      "423603/423603 [==============================] - 100s 235us/sample - loss: 0.3157\n",
      "Epoch 91/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.3151\n",
      "Epoch 92/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.3144\n",
      "Epoch 93/150\n",
      "423603/423603 [==============================] - 100s 235us/sample - loss: 0.3138\n",
      "Epoch 94/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.3132\n",
      "Epoch 95/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.3125\n",
      "Epoch 96/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.3120\n",
      "Epoch 97/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.3114\n",
      "Epoch 98/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.3108\n",
      "Epoch 99/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.3102\n",
      "Epoch 100/150\n",
      "423603/423603 [==============================] - 100s 237us/sample - loss: 0.3097\n",
      "Epoch 101/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.3091\n",
      "Epoch 102/150\n",
      "423603/423603 [==============================] - 102s 240us/sample - loss: 0.3086\n",
      "Epoch 103/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.3081\n",
      "Epoch 104/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.3076\n",
      "Epoch 105/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.3071\n",
      "Epoch 106/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.3067\n",
      "Epoch 107/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.3062\n",
      "Epoch 108/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.3057\n",
      "Epoch 109/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.3053\n",
      "Epoch 110/150\n",
      "423603/423603 [==============================] - 102s 240us/sample - loss: 0.3048\n",
      "Epoch 111/150\n",
      "423603/423603 [==============================] - 99s 235us/sample - loss: 0.3044\n",
      "Epoch 112/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.3040\n",
      "Epoch 113/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.3036\n",
      "Epoch 114/150\n",
      "423603/423603 [==============================] - 100s 237us/sample - loss: 0.3031\n",
      "Epoch 115/150\n",
      "423603/423603 [==============================] - 100s 237us/sample - loss: 0.3027\n",
      "Epoch 116/150\n",
      "423603/423603 [==============================] - 100s 237us/sample - loss: 0.3023\n",
      "Epoch 117/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.3020\n",
      "Epoch 118/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.3016\n",
      "Epoch 119/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.3012\n",
      "Epoch 120/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.3008\n",
      "Epoch 121/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.3004\n",
      "Epoch 122/150\n",
      "423603/423603 [==============================] - 100s 235us/sample - loss: 0.3001\n",
      "Epoch 123/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.2997\n",
      "Epoch 124/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.2994\n",
      "Epoch 125/150\n",
      "423603/423603 [==============================] - 100s 235us/sample - loss: 0.2991\n",
      "Epoch 126/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.2987\n",
      "Epoch 127/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.2984\n",
      "Epoch 128/150\n",
      "423603/423603 [==============================] - 100s 235us/sample - loss: 0.2981\n",
      "Epoch 129/150\n",
      "423603/423603 [==============================] - 100s 235us/sample - loss: 0.2978\n",
      "Epoch 130/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.2974\n",
      "Epoch 131/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.2971\n",
      "Epoch 132/150\n",
      "423603/423603 [==============================] - 100s 237us/sample - loss: 0.2968\n",
      "Epoch 133/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.2965\n",
      "Epoch 134/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.2962\n",
      "Epoch 135/150\n",
      "423603/423603 [==============================] - 100s 237us/sample - loss: 0.2959\n",
      "Epoch 136/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.2956\n",
      "Epoch 137/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.2954\n",
      "Epoch 138/150\n",
      "423603/423603 [==============================] - 100s 237us/sample - loss: 0.2951\n",
      "Epoch 139/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.2948\n",
      "Epoch 140/150\n",
      "423603/423603 [==============================] - 100s 235us/sample - loss: 0.2945\n",
      "Epoch 141/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.2942\n",
      "Epoch 142/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.2940\n",
      "Epoch 143/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.2937\n",
      "Epoch 144/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.2935\n",
      "Epoch 145/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.2932\n",
      "Epoch 146/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.2929\n",
      "Epoch 147/150\n",
      "423603/423603 [==============================] - 100s 235us/sample - loss: 0.2927\n",
      "Epoch 148/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.2925\n",
      "Epoch 149/150\n",
      "423603/423603 [==============================] - 101s 239us/sample - loss: 0.2922\n",
      "Epoch 150/150\n",
      "423603/423603 [==============================] - 100s 236us/sample - loss: 0.2920\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f5395679048>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.fit(x=X,y=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.model.save('model3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regressor.fit(x=X_train,y=Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters = {'batch_size': [10,100],\n",
    "#              'epochs': [100,500]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_search = GridSearchCV(estimator=regressor,\n",
    "#                           param_grid=parameters,\n",
    "#                           cv=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_search = grid_search.fit(X=X_train,y=Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch=100,epochs=200,genpact score=58\n",
    "msle = cross_val_score(estimator=regressor, X=X_train, y=Y_train, cv=8, n_jobs= -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch=10,epochs=100,genpact score=\n",
    "msle2 = cross_val_score(estimator=regressor, X=X_train, y=Y_train, cv=8, n_jobs= -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msle2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Week And Different Tuning get score 103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = regressor.predict(new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id,num_orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(Y_pred,columns=['num_orders'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.concat([test['id'],submission],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[['id']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Predition (Y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred2 = regressor.predict(new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission2 = pd.DataFrame(Y_pred2,columns=['num_orders'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission2 = pd.concat([test['id'],submission2],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission2.to_csv('submission2.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How To Save And Reload Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **SAVING THE MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My model name is regressor and I am using model.save function to the model\n",
    "# in HDF5 format.\n",
    "regressor.model.save('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **RELOADING THE MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reassigning the model after loading the model\n",
    "model = load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with batch=10, epochs=100\n",
    "regressor.model.save('model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = regressor.predict(new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = load_model('model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred2 = model2.predict(new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
